# NLP知识

### 自编码和自回归模型

![1564909673565](C:\Users\tbc\AppData\Roaming\Typora\typora-user-images\1564909673565.png)

如上所示分别为自回归模型与自编码模型，其中黄色块为输入字符，蓝色块为字符的位置。对于自回归语言模型，它希望通过已知的前半句预测后面的词或字。对于自编码语言模型，它希望通过一句话预测被 Mask 掉的字或词，如上所示第 2 个位置的词希望通过第 1、3、5 个词进行预测。

以前，最常见的语言模型就是自回归式的了，它的计算效率比较高，且明确地建模了概率密度。但是自回归语言模型有一个缺陷，它只能编码单向语义，不论是从左到右还是从右到左都只是单向语义。这对于下游 NLP 任务来说是致命的，因此也就有了 BERT 那种自编码语言模型。

BERT 通过预测被 Mask 掉的字或词，从而学习到双向语义信息。但这种任务又带来了新问题，它只是建模了近似的概率密度，因为 BERT 假设要预测的词之间是相互独立的，即 Mask 之间相互不影响。此外，自编码语言模型在预训练过程中会使用 MASK 符号，但在下游 NLP 任务中并不会使用，因此这也会造成一定的误差。

从而将上面两类模型的优点结合起来。XLNet 采用了一种新型语言建模任务，它通过随机排列自然语言而预测某个位置可能出现的词。如下图所示为排列语言模型的预测方式：

![](D:\md_images\xlnet2.jpg)

这两件事是有冲突的，如果模型需要预测位置 2 的「喜欢」，那么肯定不能用该位置的内容向量。但与此同时，位置 2 的完整向量还需要参与位置 5 的预测，且同时不能使用位置 5 的内容向量。

这类似于条件句：如果模型预测当前词，则只能使用位置向量；如果模型预测后续的词，那么使用位置加内容向量。因此这就像我们既需要标准 Transformer 提供内容向量，又要另一个网络提供对应的位置向量。

针对这种特性，研究者提出了 Two-Stream Self-Attention，它通过构建两条路径解决这个条件句。如下所示为 Two-Stream 的结构，其中左上角的 a 为 Content 流，左下角的 b 为 Query 流，右边的 c 为排列语言模型的整体建模过程。

![](D:\md_images\xlnet3.jpg)

在 Content 流中，它和标准的 Transformer 是一样的，第 1 个位置的隐藏向量 h_1 同时编码了内容与位置。在 Query 流中，第 1 个位置的隐向量 g_1 只编码了位置信息，但它同时还需要利用其它 Token 的内容隐向量 h_2、h_3 和 h_4，它们都通过 Content 流计算得出。因此，我们可以直观理解为，Query 流就是为了预测当前词，而 Content 流主要为 Query 流提供其它词的内容向量。

上图 c 展示了 XLNet 的完整计算过程，e 和 w 分别是初始化的词向量的 Query 向量。注意排列语言模型的分解顺序是 3、2、4、1，因此 Content 流的 Mask 第一行全都是红色、第二行中间两个是红色，这表明 h_1 需要用所有词信息、h_2 需要用第 2 和 3 个词的信息。此外，Query 流的对角线都是空的，表示它们不能使用自己的内容向量 h。

### 矩阵的「秩」

- 「秩」是图像经过矩阵变换之后的空间维度

  「秩」是列空间的维度

- 



### PCA  LDA

- PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。

[PCA LDA博客](https://blog.csdn.net/kuweicai/article/details/79255270)

![协方差矩阵](D:\md_images\协方差矩阵.jpg)

对角线上分别是x和y的方差，非对角线上是协方差。协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。协方差是没有单位的量，因此，如果同样的两个变量所采用的量纲发生变化，它们的协方差也会产生数值上的变化。

第三步，求协方差的特征值和特征向量，得到

![](https://img-blog.csdn.net/20150304201031902)

上面是两个特征值，下面是对应的特征向量，特征值0.0490833989对应特征向量为(-0.735178656, 0.677873399)，这里的特征向量都归一化为单位向量。

第四步，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。

这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是(-0.677873399, -0.735178656)T。

第五步，将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为

FinalData(10*1) = DataAdjust(10*2矩阵) x 特征向量(-0.677873399, -0.735178656)T

得到结果为：

![](https://img-blog.csdn.net/20150304201345746)
--------------------- 