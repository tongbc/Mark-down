# 机器学习知识

## 模型

### 1.随机森林

随机森林是一个用随机方式建立的，包含多个决策树的集成分类器。其输出的类别由各个树投票而定（如果是回归树则取平均）。随机森林的**生成过程**如下：

1. 从原始样本中采取有放回抽样的方法选取n个样本；
2. 对n个样本选取a个特征中的随机k个，用建立决策树的方法获得最佳分割点；
3. 重复m次，获得m个决策树；
4. 对输入样例进行预测时，每个子树都产生一个结果，采用多数投票机制输出。

随机森林的**随机性**主要体现在两个方面：

1. **数据集的随机选取**：从原始的数据集中采取有放回的抽样（bagging），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。
2. **待选特征的随机选取**：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。

以上两个随机性能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。

随机森林的**优点**：

1. 实现简单，训练速度快，可以并行实现，因为训练时树与树之间是相互独立的；
2. 相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
3. 能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
4. 对于不平衡的数据集，可以平衡误差；
5. 相比SVM，对特征缺失不敏感，因为待选特征也是随机选取；
6. 训练完成后可以给出哪些特征比较重要。

随机森林的**缺点**：

1. 在噪声过大的分类和回归问题还是容易过拟合；
2. 相比于单一决策树，它的随机性让我们难以对模型进行解释。

- **ID3算法**

  我们既然希望划分之后结点的“纯度”越来越高，那么如何度量纯度呢？

  **“信息熵”是度量样本集合不确定度（纯度）的最常用的指标。**

  **在我们的ID3算法中，我们采取信息增益这个量来作为纯度的度量。我们选取使得信息增益最大的特征进行分裂！那么信息增益又是什么概念呢？**

  我们前面说了，**信息熵是代表随机变量的复杂度（不确定度）通俗理解信息熵 - 知乎专栏，条件熵代表在某一个条件下，随机变量的复杂度（不确定度）通俗理解条件熵 - 知乎专栏。**

  **而我们的信息增益恰好是：信息熵-条件熵。**

  **我们看如下定义：**

  •当前样本集合 D 中第 k 类样本所占的比例为 pk ，则 D 的信息熵定义为

  ![img](https://pic1.zhimg.com/80/v2-92f9cbed49cd45d001b200b3a5cb23dc_hd.png)

  •离散属性 a 有 V 个可能的取值 {a1,a2,…,aV}；样本集合中，属性 a 上取值为 av 的样本集合，记为 Dv。

  •用属性 a 对样本集 D 进行划分所获得的“

  信息增益”

  ![img](https://pic3.zhimg.com/80/v2-23b3766e28e441c85bbe545838a1c62e_hd.jpg)

  •信息增益表示得知属性 a 的信息而使得样本集合不确定度减少的程度

  那么我们现在也很好理解了**，在决策树算法中，我们的关键就是每次选择一个特征，特征有多个，那么到底按照什么标准来选择哪一个特征。**

  **这个问题就可以用信息增益来度量。**如果选择一个特征后，**信息增益最大**（**信息不确定性减少的程度最大**），**那么我们就选取这个特征。**

  **好的，我们现在已经知道了选择指标了，就是在所有的特征中，选择信息增益最大的特征。那么如何计算呢？看下面例子：**

  ![img](https://pic1.zhimg.com/80/v2-4fd85ed2735866e05bc445f6964c1cf4_hd.png)

  正例(好瓜)占 8/17，反例占 9/17 ，根结点的信息熵为

  ![img](https://pic2.zhimg.com/80/v2-43e492644c55b02e9ee98f6b404294c5_hd.png)

  计算当前

  属性集合

  {色泽，根蒂，敲声，纹理，脐部，触感}中每个属性的信息增益

  色泽有3个可能的取值：{青绿，乌黑，浅白}

  D1(色泽=青绿) = {1, 4, 6, 10, 13, 17}，正例 3/6，反例 3/6

  D2(色泽=乌黑) = {2, 3, 7, 8, 9, 15}，正例 4/6，反例 2/6

  D3(色泽=浅白) = {5, 11, 12, 14, 16}，正例 1/5，反例 4/5

  3 个分支结点的信息熵

  ![img](https://pic2.zhimg.com/80/v2-242125ab9668562257029dbee32d254d_hd.png)

  那么我们可以知道属性色泽的信息增益是：

  ![img](https://pic3.zhimg.com/80/v2-ef0e27b825f5e09f013385f8f812a5ca_hd.png)

  同理，我们可以求出其它属性的信息增益，分别如下：

  ![img](https://pic3.zhimg.com/80/v2-31a579d93bb2ea4b8936d4c416fdb03e_hd.png)

  于是我们找到了信息增益最大的属性纹理，它的Gain(D，纹理) = 0.381最大。

  于是我们选择的划分属性为“纹理”

  如下：

  ![img](https://pic3.zhimg.com/80/v2-f782248fddd34f5ed5083f100bdd6bfe_hd.png)

  于是，我们可以得到了三个子结点，对于这三个子节点，我们可以递归的使用刚刚找信息增益最大的方法进行选择特征属性，

  比如：

  D1(纹理=清晰) = {1, 2, 3, 4, 5, 6, 8, 10, 15}，第一个分支结点可用属性集合{色泽、根蒂、敲声、脐部、触感}，基于 D1各属性的信息增益，分别求的如下：

  ![img](https://pic1.zhimg.com/80/v2-0b85f41f8a81e1ef300ca49da4619254_hd.png)

  于是我们可以选择特征属性为根蒂，脐部，触感三个特征属性中任选一个（因为他们三个相等并最大），其它俩个子结点同理，然后得到新一层的结点，再递归的由信息增益进行构建树即可

  我们最终的决策树如下：

  ![img](https://pic1.zhimg.com/80/v2-0273fa70006172ae73cb429899234afc_hd.png)

  啊，那到这里为止，我们已经知道了构建树的算法，上面也说了有了树，我们直接遍历决策树就能得到我们预测样例的类别。那么是不是大功告成了呢？

  结果是：不是的

  我们从上面求解信息增益的公式中，其实可以看出，信息增益准则其实是对可取值数目较多的属性有所偏好！

  现在假如我们把数据集中的“编号”也作为一个候选划分属性。我们可以算出“编号”的信息增益是0.998

  因为每一个样本的编号都是不同的（

  由于编号独特唯一，条件熵为0了，每一个结点中只有一类，纯度非常高啊

  ），也就是说，来了一个预测样本，你只要告诉我编号，其它特征就没有用了，

  这样生成的决策树显然不具有泛化能力。

  于是我们就引入了信息增益率来选择最优划分属性！

- **C4.5**

  这次我们每次进行选取特征属性的时候，不再使用ID3算法的信息增益，**而是使用了信息增益率这个概念。**

  首先我们来看信息增益率的公式：

  

  ![img](https://pic2.zhimg.com/80/v2-36cd923132e4a8180dba16c74c7dc9dd_hd.png)

  由上图我们可以看出，信息增益率=信息增益/IV(a),说明信息增益率是信息增益除了一个属性a的固有值得来的。

  

  **我们一开始分析到，**信息增益准则其实是对可取值数目较多的属性有所偏好！（**比如上面提到的编号，可能取值是实例个数，最多了，分的类别越多，分到每一个子结点，子结点的纯度也就越可能大，因为数量少了嘛，可能在一个类的可能性就最大**）。

  但是刚刚我们分析到了，信息增益并不是一个很好的特征选择度量。于是我们引出了信息增益率。

  我们来看IV(a)的公式：

  属性a的固有值：

  ![img](https://pic4.zhimg.com/80/v2-812104c0291d20935e910919a9fa5c27_hd.png)

  IV(触感) = 0.874 ( V = 2 )

  IV(色泽) = 1.580 ( V = 3 )

  IV(编号) = 4.088 ( V = 17 

  由上面的计算例子，可以看出IV(a)其实能够反映出，当选取该属性，分成的V类别数越大，IV(a)就越大，如果仅仅只用信息增益来选择属性的话，那么我们偏向于选择分成子节点类别大的那个特征。

  **但是在前面分析了，并不是很好，所以我们需要除以一个属性的固定值，这个值要求随着分成的类别数越大而越小。于是让它做了分母。这样可以避免信息增益的缺点。**

  那么信息增益率就是完美无瑕的吗？

  当然不是，有了这个分母之后，我们可以看到增益率准则其实对可取类别数目较少的特征有所偏好！毕竟分母越小，整体越大。

  于是C4.5算法不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性（***这样保证了大部分好的的特征***），再从中选择增益率最高的（**又保证了不会出现编号特征这种极端的情况**）希望对大家有帮助~
  
- CART

  ![1568776532381](C:\Users\tbc\AppData\Roaming\Typora\typora-user-images\1568776532381.png)

  

### 2.朴素贝叶斯分类器

- 先验概率：如果我对这个西瓜没有任何了解，包括瓜的颜色、形状、瓜蒂是否脱落。按常理来说，西瓜成熟的概率大概是 60%。那么，这个概率 P(瓜熟) 就被称为先验概率。

  先验概率是根据以往经验和分析得到的概率，先验概率无需样本数据，不受任何条件的影响。就像红色石头只根据常识而不根据西瓜状态来判断西瓜是否成熟，这就是先验概率。

- 后验概率：再来看，红色石头以前学到了一个判断西瓜是否成熟的常识，就是看瓜蒂是否脱落。一般来说，瓜蒂脱落的情况下，西瓜成熟的概率大一些，大概是 75%。如果把瓜蒂脱落当作一种结果，然后去推测西瓜成熟的概率，这个概率 P(瓜熟 | 瓜蒂脱落) 就被称为后验概率。后验概率类似于条件概率。

- 联合概率：红色石头买西瓜的例子中，P(瓜熟，瓜蒂脱落) 称之为联合分布，它表示瓜熟了且瓜蒂脱落的概率。关于联合概率，满足下列乘法等式：

  P（瓜熟，瓜地脱落）  = P（瓜熟|瓜地脱落）*P（瓜地脱落） = P（瓜地脱落|瓜熟）* P（瓜熟）

### 3.感知机

[知乎1](https://zhuanlan.zhihu.com/p/25696112)

[知乎2](https://zhuanlan.zhihu.com/p/25696112)

![](https://pic1.zhimg.com/80/v2-2845550e373e17d02e16c5e8f161a460_hd.png)



```python
# -*- coding: utf-8 -*-
import copy

trainint_set = [[(3,3),1],[(4,3),1],[(1,1),-1]]       #输入数据
w = [0,0]          #初始化w参数
b = 0              #初始化b参数

def update(item):
    global w,b
    w[0] += 1*item[1]*item[0][0]               #w的第一个分量更新
    w[1] += 1*item[1]*item[0][1]               #w的第二个分量更新
    b += 1*item[1]
    print 'w = ',w,'b=',b                     #打印出结果

def judge(item):                               #返回y = yi(w*x+b)的结果
    res = 0
    for i in range(len(item[0])):
        res +=item[0][i]*w[i]                   #对应公式w*x
    res += b                                    #对应公式w*x+b
    res *= item[1]                              #对应公式yi(w*x+b)
    return res

def check():                                    #检查所有数据点是否分对了
    flag = False
    for item in trainint_set:
        if judge(item)<=0:                       #如果还有误分类点，那么就小于等于0
            flag = True
            update(item)                         #只要有一个点分错，我就更新
    return flag                                  #flag为False，说明没有分错的了

if __name__ == '__main__':
    flag = False
    for i in range(1000):
        if not check():                            #如果已经没有分错的话
            flag = True
            break
    if flag:
        print "在1000次以内全部分对了"
    else:
        print "很不幸，1000次迭代还是没有分对"

```

### 4.SVM

[石头老师 林轩田](https://blog.csdn.net/red_stone1/article/details/73526457)

[对偶svm](https://blog.csdn.net/red_stone1/article/details/73822768)

现在，若要计算平面外一点x到该平面的距离，做法是只要将向量(x-x’)投影到垂直于该平面的方向（即w方向）上就可以了。那么，令(x”-x’)与w的夹角为θ，距离就可以表示为：
$$
distance(x,b,w)=|(x-x')cos(\theta)|=|\ ||x-x'||\cdot \frac{(x-x')w}{||x-x'||\cdot ||w||}|=\frac1{||w||}|w^Tx-w^Tx'|
$$
带入
$$
w^Tx'=-b
$$
得到
$$
distance(x,b,w)=\frac1{||w||}|w^Tx+b|
$$
则distance公式就可以变换成
$$
distance(x,b,w)=\frac1{||w||}y_n(w^Tx_n+b)
$$

https://zhuanlan.zhihu.com/p/49331510

### 5.逻辑回归

[link](https://zhuanlan.zhihu.com/p/77868008)

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(t):
    return 1 / (1 + np.exp(-t))

x = np.linspace(-10, 10, 500)
y = sigmoid(x)
plt.plot(x, y)
plt.show()
```

https://zhuanlan.zhihu.com/p/44591359

### 6.PCA

[llink](https://zhuanlan.zhihu.com/p/77151308)

1. 内积：内积运算将两个向量映射为实数，其计算方式非常容易理解，但我们无法看出其物理含义。接下来我们从几何角度来分析，为了简单起见，我们假设 A 和 B 均为二维向量，则：

   ![](https://www.zhihu.com/equation?tex=A%3D%28x_1%2Cy_1%29%EF%BC%8CB%3D%28x_2%2Cy_2%29+%5C+A+%5Ccdot+B+%3D+%7CA%7C%7CB%7Ccos%28%5Calpha%29+%5C%5C)

   我们看出 A 与 B 的内积等于 A 到 B 的投影长度乘以 B 的模。

   如果假设 B 的模为 1，即让 ![[公式]](https://www.zhihu.com/equation?tex=%7CB%7C%3D1) ，那么就变成了：

   ![](https://www.zhihu.com/equation?tex=A%5Ccdot+B%3D%7CA%7Ccos%28a%29+%5C%5C)

   也就是说，**A 与 B 的内积值等于 A 向 B 所在直线投影的矢量长度。**

2. 协方差：在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，协方差可以表示两个变量的相关性。为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。

   协方差公式为

   ![](https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm-1%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu_a%29%28b_i-%5Cmu_b%29%7D+%5C%5C)

   当协方差为 0 时，表示两个变量完全独立。为了让协方差为 0，我们选择第二个基时只能在与第一个基正交的方向上进行选择，因此最终选择的两个方向一定是正交的。

   至此，我们得到了降维问题的优化目标：**将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。**

3. 协方差矩阵：针对我们给出的优化目标，接下来我们将从数学的角度来给出优化目标。

   我们看到，最终要达到的目的与**变量内方差及变量间协方差**有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们有：

   假设我们只有 a 和 b 两个变量，那么我们将它们按行组成矩阵 X：

   ![](https://www.zhihu.com/equation?tex=X%3D%5Cbegin%7Bpmatrix%7D++a_1+%26+a_2+%26+%5Ccdots+%26+a_m+%5C%5C+b_1+%26+b_2+%26+%5Ccdots+%26+b_m++%5Cend%7Bpmatrix%7D+%5C%5C)

   ![](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D%3D+%5Cbegin%7Bpmatrix%7D++%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Bb_i%5E2%7D++%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D++Cov%28a%2Ca%29+%26+Cov%28a%2Cb%29+%5C%5C++Cov%28b%2Ca%29+%26+Cov%28b%2Cb%29+%5Cend%7Bpmatrix%7D+%5C%5C)

   我们可以看到这个矩阵对角线上的分别是两个变量的方差，而其它元素是 a 和 b 的协方差。两者被统一到了一个矩阵里。

   **设我们有 m 个 n 维数据记录，将其排列成矩阵** ![[公式]](https://www.zhihu.com/equation?tex=X_%7Bn%2Cm%7D) **，设** ![[公式]](https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5ET) **，则 C 是一个对称矩阵，其对角线分别对应各个变量的方差，而第 i 行 j 列和 j 行 i 列元素相同，表示 i 和 j 两个变量的协方差**。