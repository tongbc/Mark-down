# 机器学习知识

## 模型

### 1.随机森林

随机森林是一个用随机方式建立的，包含多个决策树的集成分类器。其输出的类别由各个树投票而定（如果是回归树则取平均）。随机森林的**生成过程**如下：

1. 从原始样本中采取有放回抽样的方法选取n个样本；
2. 对n个样本选取a个特征中的随机k个，用建立决策树的方法获得最佳分割点；
3. 重复m次，获得m个决策树；
4. 对输入样例进行预测时，每个子树都产生一个结果，采用多数投票机制输出。

随机森林的**随机性**主要体现在两个方面：

1. **数据集的随机选取**：从原始的数据集中采取有放回的抽样（bagging），构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。
2. **待选特征的随机选取**：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。

以上两个随机性能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。

随机森林的**优点**：

1. 实现简单，训练速度快，可以并行实现，因为训练时树与树之间是相互独立的；
2. 相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
3. 能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
4. 对于不平衡的数据集，可以平衡误差；
5. 相比SVM，对特征缺失不敏感，因为待选特征也是随机选取；
6. 训练完成后可以给出哪些特征比较重要。

随机森林的**缺点**：

1. 在噪声过大的分类和回归问题还是容易过拟合；
2. 相比于单一决策树，它的随机性让我们难以对模型进行解释。

- **ID3算法**

  我们既然希望划分之后结点的“纯度”越来越高，那么如何度量纯度呢？

  **“信息熵”是度量样本集合不确定度（纯度）的最常用的指标。**

  **在我们的ID3算法中，我们采取信息增益这个量来作为纯度的度量。我们选取使得信息增益最大的特征进行分裂！那么信息增益又是什么概念呢？**

  我们前面说了，**信息熵是代表随机变量的复杂度（不确定度）通俗理解信息熵 - 知乎专栏，条件熵代表在某一个条件下，随机变量的复杂度（不确定度）通俗理解条件熵 - 知乎专栏。**

  **而我们的信息增益恰好是：信息熵-条件熵。**

  **我们看如下定义：**

  •当前样本集合 D 中第 k 类样本所占的比例为 pk ，则 D 的信息熵定义为

  ![img](https://pic1.zhimg.com/80/v2-92f9cbed49cd45d001b200b3a5cb23dc_hd.png)

  •离散属性 a 有 V 个可能的取值 {a1,a2,…,aV}；样本集合中，属性 a 上取值为 av 的样本集合，记为 Dv。

  •用属性 a 对样本集 D 进行划分所获得的“

  信息增益”

  ![img](https://pic3.zhimg.com/80/v2-23b3766e28e441c85bbe545838a1c62e_hd.jpg)

  •信息增益表示得知属性 a 的信息而使得样本集合不确定度减少的程度

  那么我们现在也很好理解了**，在决策树算法中，我们的关键就是每次选择一个特征，特征有多个，那么到底按照什么标准来选择哪一个特征。**

  **这个问题就可以用信息增益来度量。**如果选择一个特征后，**信息增益最大**（**信息不确定性减少的程度最大**），**那么我们就选取这个特征。**

  **好的，我们现在已经知道了选择指标了，就是在所有的特征中，选择信息增益最大的特征。那么如何计算呢？看下面例子：**

  ![img](https://pic1.zhimg.com/80/v2-4fd85ed2735866e05bc445f6964c1cf4_hd.png)

  正例(好瓜)占 8/17，反例占 9/17 ，根结点的信息熵为

  ![img](https://pic2.zhimg.com/80/v2-43e492644c55b02e9ee98f6b404294c5_hd.png)

  计算当前

  属性集合

  {色泽，根蒂，敲声，纹理，脐部，触感}中每个属性的信息增益

  色泽有3个可能的取值：{青绿，乌黑，浅白}

  D1(色泽=青绿) = {1, 4, 6, 10, 13, 17}，正例 3/6，反例 3/6

  D2(色泽=乌黑) = {2, 3, 7, 8, 9, 15}，正例 4/6，反例 2/6

  D3(色泽=浅白) = {5, 11, 12, 14, 16}，正例 1/5，反例 4/5

  3 个分支结点的信息熵

  ![img](https://pic2.zhimg.com/80/v2-242125ab9668562257029dbee32d254d_hd.png)

  那么我们可以知道属性色泽的信息增益是：

  ![img](https://pic3.zhimg.com/80/v2-ef0e27b825f5e09f013385f8f812a5ca_hd.png)

  同理，我们可以求出其它属性的信息增益，分别如下：

  ![img](https://pic3.zhimg.com/80/v2-31a579d93bb2ea4b8936d4c416fdb03e_hd.png)

  于是我们找到了信息增益最大的属性纹理，它的Gain(D，纹理) = 0.381最大。

  于是我们选择的划分属性为“纹理”

  如下：

  ![img](https://pic3.zhimg.com/80/v2-f782248fddd34f5ed5083f100bdd6bfe_hd.png)

  于是，我们可以得到了三个子结点，对于这三个子节点，我们可以递归的使用刚刚找信息增益最大的方法进行选择特征属性，

  比如：

  D1(纹理=清晰) = {1, 2, 3, 4, 5, 6, 8, 10, 15}，第一个分支结点可用属性集合{色泽、根蒂、敲声、脐部、触感}，基于 D1各属性的信息增益，分别求的如下：

  ![img](https://pic1.zhimg.com/80/v2-0b85f41f8a81e1ef300ca49da4619254_hd.png)

  于是我们可以选择特征属性为根蒂，脐部，触感三个特征属性中任选一个（因为他们三个相等并最大），其它俩个子结点同理，然后得到新一层的结点，再递归的由信息增益进行构建树即可

  我们最终的决策树如下：

  ![img](https://pic1.zhimg.com/80/v2-0273fa70006172ae73cb429899234afc_hd.png)

  啊，那到这里为止，我们已经知道了构建树的算法，上面也说了有了树，我们直接遍历决策树就能得到我们预测样例的类别。那么是不是大功告成了呢？

  结果是：不是的

  我们从上面求解信息增益的公式中，其实可以看出，信息增益准则其实是对可取值数目较多的属性有所偏好！

  现在假如我们把数据集中的“编号”也作为一个候选划分属性。我们可以算出“编号”的信息增益是0.998

  因为每一个样本的编号都是不同的（

  由于编号独特唯一，条件熵为0了，每一个结点中只有一类，纯度非常高啊

  ），也就是说，来了一个预测样本，你只要告诉我编号，其它特征就没有用了，

  这样生成的决策树显然不具有泛化能力。

  于是我们就引入了信息增益率来选择最优划分属性！

- **C4.5**

  这次我们每次进行选取特征属性的时候，不再使用ID3算法的信息增益，**而是使用了信息增益率这个概念。**

  首先我们来看信息增益率的公式：

  

  ![img](https://pic2.zhimg.com/80/v2-36cd923132e4a8180dba16c74c7dc9dd_hd.png)

  由上图我们可以看出，信息增益率=信息增益/IV(a),说明信息增益率是信息增益除了一个属性a的固有值得来的。

  

  **我们一开始分析到，**信息增益准则其实是对可取值数目较多的属性有所偏好！（**比如上面提到的编号，可能取值是实例个数，最多了，分的类别越多，分到每一个子结点，子结点的纯度也就越可能大，因为数量少了嘛，可能在一个类的可能性就最大**）。

  但是刚刚我们分析到了，信息增益并不是一个很好的特征选择度量。于是我们引出了信息增益率。

  我们来看IV(a)的公式：

  属性a的固有值：

  ![img](https://pic4.zhimg.com/80/v2-812104c0291d20935e910919a9fa5c27_hd.png)

  IV(触感) = 0.874 ( V = 2 )

  IV(色泽) = 1.580 ( V = 3 )

  IV(编号) = 4.088 ( V = 17 

  由上面的计算例子，可以看出IV(a)其实能够反映出，当选取该属性，分成的V类别数越大，IV(a)就越大，如果仅仅只用信息增益来选择属性的话，那么我们偏向于选择分成子节点类别大的那个特征。

  **但是在前面分析了，并不是很好，所以我们需要除以一个属性的固定值，这个值要求随着分成的类别数越大而越小。于是让它做了分母。这样可以避免信息增益的缺点。**

  那么信息增益率就是完美无瑕的吗？

  当然不是，有了这个分母之后，我们可以看到增益率准则其实对可取类别数目较少的特征有所偏好！毕竟分母越小，整体越大。

  于是C4.5算法不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性（***这样保证了大部分好的的特征***），再从中选择增益率最高的（**又保证了不会出现编号特征这种极端的情况**）希望对大家有帮助~